# bearing_dataset.py

import os
import torch
import numpy as np
import pandas as pd
from scipy.io import loadmat
from scipy.signal import butter, filtfilt
from joblib import dump, load
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset

# === ç¬¬ä¸€éƒ¨åˆ†ï¼šä» MAT æ–‡ä»¶ç”Ÿæˆ CSVï¼Œå¹¶åˆ’åˆ†ä¿å­˜ä¸º .joblib ===

# ç›®å½• & æ–‡ä»¶å è®¾ç½®
BASE_DIR = 'dataset/matfiles'
MAT_FILES = [
    '0_0.mat', '7_1.mat', '7_2.mat', '7_3.mat',
    '14_1.mat', '14_2.mat', '14_3.mat',
    '21_1.mat', '21_2.mat', '21_3.mat'
]
MAT_FIELDS = [
    'X097_DE_time', 'X105_DE_time', 'X118_DE_time', 'X130_DE_time',
    'X169_DE_time', 'X185_DE_time', 'X197_DE_time', 'X209_DE_time',
    'X222_DE_time', 'X234_DE_time'
]
COLUMN_NAMES = [
    'de_normal', 'de_7_inner', 'de_7_ball', 'de_7_outer',
    'de_14_inner', 'de_14_ball', 'de_14_outer',
    'de_21_inner', 'de_21_ball', 'de_21_outer'
]
# æ ‡ç­¾æ˜ å°„
# â€”â€” 10 ç±»ä¸€ä¸€æ˜ å°„ â€”â€” 
LABEL_MAP = {
    'de_normal'    : 0,
    'de_7_inner'   : 1,
    'de_7_ball'    : 2,
    'de_7_outer'   : 3,
    'de_14_inner'  : 4,
    'de_14_ball'   : 5,
    'de_14_outer'  : 6,
    'de_21_inner'  : 7,
    'de_21_ball'   : 8,
    'de_21_outer'  : 9,
}

def bandpass_filter(data, lowcut=200, highcut=5900, fs=12000, order=4):
    """å¸¦é€šæ»¤æ³¢"""
    nyq = 0.5 * fs
    b, a = butter(order, [lowcut/nyq, highcut/nyq], btype='band')
    return filtfilt(b, a, data)

def normalize(data):
    """0â€“1 å½’ä¸€åŒ–"""
    mn, mx = data.min(), data.max()
    return (data - mn) / (mx - mn + 1e-8)

def create_dualtask_samples(signal, label, Lhist=1024, Lpred=1024, stride=512):
    """
    ä»ä¸€æ¡å½’ä¸€åŒ–åçš„åºåˆ—ç”Ÿæˆå¤šä»»åŠ¡æ ·æœ¬ï¼š
      x: Lhist ç‚¹å†å²ï¼› y: Lpred ç‚¹æœªæ¥ï¼› æœ€åé™„å¸¦ label
    è¿”å› samples[N, Lhist+Lpred+1] å’Œ condition[N]
    """
    L = Lhist + Lpred
    samples = []
    for i in range(0, len(signal) - L + 1, stride):
        x = signal[i:i+Lhist]
        y = signal[i+Lhist:i+L]
        samples.append(np.concatenate([x, y, [label]]))
    return np.stack(samples, axis=0)

def process_dataset(csv_file='data_12k_10c.csv',
                    Lhist=1024, Lpred=1024, stride=512,
                    split_rate=(0.6, 0.2, 0.2)):
    """
    1) è¯»å– CSV
    2) å¯¹æ¯ä¸€åˆ—æŒ‰ LABEL_MAP å¤„ç†ã€æ»¤æ³¢ã€åˆ‡ç‰‡æ„é€ æ ·æœ¬
    3) åˆ†å±‚æŠ½æ ·åˆ’åˆ† train/val/test
    4) ä¿å­˜å½’ä¸€åŒ–åå¼ é‡åˆ° .joblibï¼š Xã€Yclassã€Ytrend
    """

    df = pd.read_csv(csv_file)
    all_samples, all_labels = [], []
    
    for col in df.columns:
        if col not in LABEL_MAP: continue
        sig = df[col].values
        sig = normalize(bandpass_filter(sig))
        lab = LABEL_MAP[col]
        samp = create_dualtask_samples(sig, lab, Lhist, Lpred, stride)
        all_samples.append(samp)
        all_labels += [lab] * len(samp)

    data = np.vstack(all_samples)  # [æ€»æ ·æœ¬, Lhist+Lpred+1]
    # æœ€åä¸€åˆ—æ˜¯ label
    X = data[:, :Lhist]
    Ytrend = data[:, Lhist:Lhist+Lpred]
    Yclass = data[:, -1].astype(np.int64)

    # åˆ†å±‚æŠ½æ · split
    idx = np.arange(len(X))
    train_idx, tmp = train_test_split(idx, test_size=split_rate[1]+split_rate[2],
                                       stratify=Yclass, random_state=42)
    val_idx, test_idx = train_test_split(tmp, test_size=split_rate[2]/(split_rate[1]+split_rate[2]),
                                         stratify=Yclass[tmp], random_state=42)

    splits = {
        'train': train_idx,
        'val':   val_idx,
        'test':  test_idx
    }

    # ä¿å­˜
    for split, ids in splits.items():
        dump(X[ids],       f'{split}X_dualtask.joblib')
        dump(Yclass[ids],  f'{split}Yclass_dualtask.joblib')
        dump(Ytrend[ids],  f'{split}Ytrend_dualtask.joblib')
        print(f"âœ… {split} é›†: æ ·æœ¬æ•° {len(ids)}, æ–‡ä»¶ {split}X/Yclass/Ytrend_dualtask.joblib")

    # æ‰“å°æ€»æ ·æœ¬ç»Ÿè®¡    
    total_samples = len(X)
    print(f"\nğŸ“Š æ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"â¡ï¸ è®­ç»ƒé›†: {len(train_idx)} æ¡")
    print(f"â¡ï¸ éªŒè¯é›†: {len(val_idx)} æ¡")
    print(f"â¡ï¸ æµ‹è¯•é›†: {len(test_idx)} æ¡")

if __name__ == "__main__":
    # å…ˆç”Ÿæˆ CSVï¼ˆå¦‚æœå·²å­˜åœ¨ï¼Œå¯æ³¨é‡Šæ‰è¿™ä¸€å—ï¼‰
    # -------------------------------------------------------
    # data_12k_10c = pd.DataFrame()
    # for i, fn in enumerate(MAT_FILES):
    #     mat = loadmat(os.path.join(BASE_DIR, fn))
    #     arr = mat[MAT_FIELDS[i]].reshape(-1)[:119808]
    #     data_12k_10c[COLUMN_NAMES[i]] = arr
    # data_12k_10c.to_csv('data_12k_10c.csv', index=False)
    # print("âœ… data_12k_10c.csv å·²ç”Ÿæˆ")
    # -------------------------------------------------------
    # åˆ’åˆ†å¹¶ä¿å­˜ .joblib
    process_dataset()

# === ç¬¬äºŒéƒ¨åˆ†ï¼šPyTorch Dataset å°è£… ===

class BearingDataset(Dataset):
    """
    ä»ä¸Šé¢ä¿å­˜çš„ .joblib æ–‡ä»¶ä¸­åŠ è½½ dual-task æ•°æ®ã€‚
    è¿”å›ï¼š(X_tensor [1,Lhist], y_class_tensor, y_trend_tensor [Lpred])
    """

    def __init__(self, data_dir='.', split='train'):
        assert split in ('train','val','test')
        fX, fC, fT = (
            f'{split}X_dualtask.joblib',
            f'{split}Yclass_dualtask.joblib',
            f'{split}Ytrend_dualtask.joblib'
        )
        self.X  = torch.from_numpy(load(os.path.join(data_dir, fX))).float()    # [N, Lhist]
        self.yc = torch.from_numpy(load(os.path.join(data_dir, fC))).long()     # [N]
        self.yt = torch.from_numpy(load(os.path.join(data_dir, fT))).float()    # [N, Lpred]

        # å¦‚æœä½ çš„æ¨¡å‹æœŸæœ› [batch,1,seq_len]ï¼Œå¯ä»¥åœ¨ __getitem__ é‡Œ unsqueeze
        assert len(self.X)==len(self.yc)==len(self.yt)

    def __len__(self):
        return self.X.size(0)

    def __getitem__(self, idx):
        # åœ¨æœ€åä¸€ç»´å¢åŠ é€šé“ï¼šè¾“å‡º (Lhist, 1)ï¼ŒDataLoader æ‰¹é‡åå³ (batch, Lhist, 1)
        x = self.X[idx].unsqueeze(-1)
        return x, self.yc[idx], self.yt[idx]
